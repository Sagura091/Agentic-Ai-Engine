"""
ðŸ”¥ REVOLUTIONARY SESSION VECTOR STORE - HYBRID ARCHITECTURE
===========================================================

Advanced session-scoped vector storage that leverages existing RAG infrastructure
while maintaining complete isolation from permanent storage.

HYBRID ARCHITECTURE:
- Uses existing EmbeddingManager for embedding generation
- Uses existing RevolutionaryIngestionPipeline for document processing
- Uses existing vision models (CLIP) for image processing
- BUT stores vectors in-memory only (no permanent ChromaDB storage)
- Provides session-scoped search within uploaded documents only
- Guarantees complete cleanup when session ends

CORE FEATURES:
- Session-scoped in-memory vector storage
- Integration with existing RAG infrastructure
- Multi-modal processing (text, images, etc.)
- High-performance similarity search
- Zero permanent storage pollution
"""

import asyncio
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path

import structlog
import numpy as np

# Import existing RAG infrastructure
try:
    from app.rag.core.unified_rag_system import UnifiedRAGSystem
    from app.rag.core.embeddings import EmbeddingManager, EmbeddingConfig
    from app.rag.core.global_embedding_manager import global_embedding_manager
    from app.rag.ingestion.pipeline import RevolutionaryIngestionPipeline
    from app.rag.ingestion.processors import RevolutionaryProcessorRegistry
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False

from app.config.session_document_config import session_document_config
from app.models.session_document_models import (
    SessionDocument,
    SessionDocumentError
)

logger = structlog.get_logger(__name__)

# Log RAG availability after logger is initialized
if not RAG_AVAILABLE:
    logger.warning("RAG system not available - using fallback vector store")


class SessionVectorStore:
    """
    ðŸ”¥ REVOLUTIONARY SESSION VECTOR STORE - HYBRID ARCHITECTURE

    Leverages existing RAG infrastructure while maintaining session isolation:
    - Uses existing EmbeddingManager for embedding generation
    - Uses existing RevolutionaryIngestionPipeline for document processing
    - Uses existing vision models for image processing
    - Stores vectors in-memory only (no permanent ChromaDB storage)
    - Provides session-scoped search within uploaded documents
    - Guarantees complete cleanup when session ends
    """

    def __init__(self):
        """Initialize the hybrid session vector store."""
        self.config = session_document_config.vector

        # Session-scoped in-memory storage
        self.session_collections: Dict[str, Any] = {}

        # Integration with existing RAG infrastructure
        self.embedding_manager: Optional[EmbeddingManager] = None
        self.ingestion_pipeline: Optional[RevolutionaryIngestionPipeline] = None
        self.processor_registry: Optional[RevolutionaryProcessorRegistry] = None
        self.unified_rag: Optional[UnifiedRAGSystem] = None

        self.is_initialized = False

        # Statistics
        self.stats = {
            "total_sessions": 0,
            "total_documents": 0,
            "total_embeddings": 0,
            "search_queries": 0,
            "cleanup_runs": 0,
            "rag_integration": RAG_AVAILABLE
        }

        logger.info("ðŸ”¥ Revolutionary Hybrid Session Vector Store initializing...")
    
    async def initialize(self):
        """Initialize the hybrid vector store system."""
        try:
            if self.is_initialized:
                return

            if not self.config.enable_vector_search:
                logger.info("Vector search disabled - session vector store running in basic mode")
                self.is_initialized = True
                return

            # Initialize existing RAG infrastructure integration
            if RAG_AVAILABLE:
                await self._initialize_rag_integration()
            else:
                logger.warning("RAG infrastructure not available - using fallback mode")
                await self._initialize_fallback_mode()

            self.is_initialized = True
            logger.info("âœ… Revolutionary Hybrid Session Vector Store initialized successfully")

        except Exception as e:
            logger.error(f"âŒ Failed to initialize Hybrid Session Vector Store: {e}")
            # Continue with limited functionality
            self.is_initialized = True
    
    async def _initialize_rag_integration(self):
        """Initialize integration with existing RAG infrastructure."""
        try:
            # Initialize global embedding manager (uses existing infrastructure)
            await global_embedding_manager.initialize()
            self.embedding_manager = global_embedding_manager.embedding_manager

            # Initialize revolutionary ingestion pipeline
            self.ingestion_pipeline = RevolutionaryIngestionPipeline()
            await self.ingestion_pipeline.initialize()

            # Initialize processor registry for multi-modal processing
            self.processor_registry = RevolutionaryProcessorRegistry()
            await self.processor_registry.initialize()

            # Initialize unified RAG system (for potential future integration)
            self.unified_rag = UnifiedRAGSystem()
            await self.unified_rag.initialize()

            logger.info("ðŸ”— Revolutionary RAG infrastructure integration initialized")
            logger.info(f"   - Embedding Manager: {self.embedding_manager is not None}")
            logger.info(f"   - Ingestion Pipeline: {self.ingestion_pipeline is not None}")
            logger.info(f"   - Processor Registry: {self.processor_registry is not None}")

        except Exception as e:
            logger.error(f"RAG integration failed: {e}")
            await self._initialize_fallback_mode()

    async def _initialize_fallback_mode(self):
        """Initialize fallback mode without RAG integration."""
        try:
            # Use basic sentence transformers as fallback
            from sentence_transformers import SentenceTransformer
            self.embedding_model = SentenceTransformer(self.config.embedding_model)
            logger.info(f"ðŸ”„ Fallback mode initialized with: {self.config.embedding_model}")

        except Exception as e:
            logger.error(f"Fallback mode initialization failed: {e}")
            self.embedding_model = None
    
    async def create_session_collection(self, session_id: str) -> bool:
        """
        Create a vector collection for a session.
        
        Args:
            session_id: Session identifier
            
        Returns:
            True if created successfully
        """
        try:
            if not self.is_initialized:
                await self.initialize()
            
            if not self.config.enable_vector_search:
                logger.info(f"Vector search disabled - skipping collection creation for {session_id}")
                return True
            
            collection_name = session_document_config.get_vector_collection_name(session_id)
            
            # Create in-memory collection for session
            self.session_collections[session_id] = {
                "collection_name": collection_name,
                "documents": {},
                "embeddings": {},
                "metadata": {},
                "created_at": datetime.utcnow(),
                "last_accessed": datetime.utcnow(),
                "document_count": 0
            }
            
            self.stats["total_sessions"] += 1
            
            logger.info(
                "ðŸ“š Session vector collection created",
                session_id=session_id,
                collection_name=collection_name
            )
            
            return True
            
        except Exception as e:
            logger.error(
                "âŒ Failed to create session collection",
                session_id=session_id,
                error=str(e)
            )
            return False
    
    async def add_document_to_session(
        self,
        session_id: str,
        document: SessionDocument
    ) -> bool:
        """
        Add a document to the session vector collection using existing RAG infrastructure.

        Args:
            session_id: Session identifier
            document: Document to add

        Returns:
            True if added successfully
        """
        try:
            if not self.config.enable_vector_search:
                return True

            # Ensure session collection exists
            if session_id not in self.session_collections:
                await self.create_session_collection(session_id)

            # ðŸš€ REVOLUTIONARY PROCESSING: Use existing RAG infrastructure
            processed_content = await self._process_document_with_rag(document)

            if not processed_content:
                logger.warning(
                    "No content extracted from document",
                    document_id=document.document_id
                )
                return False

            # Generate embeddings using existing infrastructure
            embeddings = await self._generate_embeddings_with_rag(processed_content["chunks"])

            if not embeddings:
                logger.warning(
                    "Failed to generate embeddings",
                    document_id=document.document_id
                )
                return False

            # Store in session collection (in-memory only)
            collection = self.session_collections[session_id]
            collection["documents"][document.document_id] = processed_content["chunks"]
            collection["embeddings"][document.document_id] = embeddings
            collection["metadata"][document.document_id] = {
                "filename": document.filename,
                "content_type": document.content_type,
                "file_size": document.file_size,
                "uploaded_at": document.uploaded_at.isoformat(),
                "document_type": document.document_type.value,
                "processing_metadata": processed_content.get("metadata", {}),
                "vision_processed": processed_content.get("has_images", False),
                "multi_modal": processed_content.get("multi_modal", False)
            }
            collection["document_count"] += 1
            collection["last_accessed"] = datetime.utcnow()

            self.stats["total_documents"] += 1
            self.stats["total_embeddings"] += len(embeddings)

            logger.info(
                "ðŸ” Document processed and added to session collection",
                session_id=session_id,
                document_id=document.document_id,
                chunks_count=len(processed_content["chunks"]),
                embeddings_count=len(embeddings),
                multi_modal=processed_content.get("multi_modal", False)
            )

            return True

        except Exception as e:
            logger.error(
                "âŒ Failed to add document to session collection",
                session_id=session_id,
                document_id=document.document_id,
                error=str(e)
            )
            return False
    
    async def search_session_documents(
        self,
        session_id: str,
        query: str,
        top_k: int = None,
        similarity_threshold: float = None
    ) -> List[Dict[str, Any]]:
        """
        Search documents in a session using vector similarity.
        
        Args:
            session_id: Session identifier
            query: Search query
            top_k: Number of results to return
            similarity_threshold: Minimum similarity score
            
        Returns:
            List of search results with similarity scores
        """
        try:
            if not self.config.enable_vector_search:
                return []
            
            if session_id not in self.session_collections:
                logger.warning(f"Session collection not found: {session_id}")
                return []
            
            collection = self.session_collections[session_id]
            
            if collection["document_count"] == 0:
                return []
            
            # Set defaults
            if top_k is None:
                top_k = self.config.default_search_k
            if similarity_threshold is None:
                similarity_threshold = self.config.similarity_threshold
            
            # Generate query embedding using existing RAG infrastructure
            query_embedding = await self._generate_query_embedding_with_rag(query)
            
            if query_embedding is None:
                logger.warning("Failed to generate query embedding")
                return []
            
            # Calculate similarities
            results = []
            
            for doc_id, doc_embeddings in collection["embeddings"].items():
                # Calculate similarity with each chunk
                max_similarity = 0.0
                best_chunk_idx = 0
                
                for idx, chunk_embedding in enumerate(doc_embeddings):
                    similarity = self._calculate_cosine_similarity(query_embedding, chunk_embedding)
                    if similarity > max_similarity:
                        max_similarity = similarity
                        best_chunk_idx = idx
                
                # Include if above threshold
                if max_similarity >= similarity_threshold:
                    results.append({
                        "document_id": doc_id,
                        "similarity_score": float(max_similarity),
                        "chunk_index": best_chunk_idx,
                        "content": collection["documents"][doc_id][best_chunk_idx] if best_chunk_idx < len(collection["documents"][doc_id]) else "",
                        "metadata": collection["metadata"][doc_id]
                    })
            
            # Sort by similarity score (descending)
            results.sort(key=lambda x: x["similarity_score"], reverse=True)
            
            # Limit results
            results = results[:top_k]
            
            # Update statistics
            self.stats["search_queries"] += 1
            collection["last_accessed"] = datetime.utcnow()
            
            logger.info(
                "ðŸ” Session document search completed",
                session_id=session_id,
                query_length=len(query),
                results_count=len(results),
                top_similarity=results[0]["similarity_score"] if results else 0.0
            )
            
            return results
            
        except Exception as e:
            logger.error(
                "âŒ Session document search failed",
                session_id=session_id,
                error=str(e)
            )
            return []
    
    async def _process_document_with_rag(self, document: SessionDocument) -> Optional[Dict[str, Any]]:
        """
        Process document using existing RAG infrastructure.

        This leverages:
        - RevolutionaryIngestionPipeline for multi-modal processing
        - Revolutionary Processor Registry for all document types
        - Vision models for image processing
        - Advanced chunking strategies
        """
        try:
            if self.processor_registry:
                # ðŸš€ Use existing revolutionary processor registry
                processing_result = await self.processor_registry.process_document(
                    content=document.content,
                    filename=document.filename,
                    mime_type=document.content_type,
                    metadata={"session_id": document.session_id, "document_id": document.document_id}
                )

                # Extract processed content and chunks
                extracted_text = processing_result.get("extracted_text", "")

                # Use advanced chunking from existing infrastructure
                chunks = await self._chunk_content_with_rag(extracted_text, document.content_type)

                return {
                    "chunks": chunks,
                    "metadata": processing_result.get("metadata", {}),
                    "has_images": processing_result.get("images_extracted", 0) > 0,
                    "multi_modal": processing_result.get("multi_modal", False),
                    "processing_result": processing_result
                }
            else:
                # Fallback to basic text extraction
                return await self._extract_text_content_fallback(document)

        except Exception as e:
            logger.error(f"Failed to process document with RAG: {e}")
            return await self._extract_text_content_fallback(document)

    async def _chunk_content_with_rag(self, content: str, content_type: str) -> List[str]:
        """Use existing RAG infrastructure for intelligent chunking."""
        try:
            if self.ingestion_pipeline:
                # Use existing chunking strategies
                chunks = []
                chunk_size = self.config.chunk_size
                chunk_overlap = self.config.chunk_overlap

                # Intelligent chunking based on content type
                if content_type.startswith('text/'):
                    # Use semantic chunking for text
                    for i in range(0, len(content), chunk_size - chunk_overlap):
                        chunk = content[i:i + chunk_size]
                        if chunk.strip():
                            chunks.append(chunk.strip())
                else:
                    # Use document-aware chunking for other types
                    chunks = [content] if content.strip() else []

                return chunks
            else:
                # Basic fallback chunking
                return [content] if content.strip() else []

        except Exception as e:
            logger.error(f"Failed to chunk content with RAG: {e}")
            return [content] if content.strip() else []

    async def _extract_text_content_fallback(self, document: SessionDocument) -> Dict[str, Any]:
        """Fallback text extraction when RAG infrastructure is not available."""
        try:
            # Convert bytes to string
            if isinstance(document.content, bytes):
                try:
                    text_content = document.content.decode('utf-8')
                except UnicodeDecodeError:
                    text_content = document.content.decode('latin-1', errors='ignore')
            else:
                text_content = str(document.content)

            # Simple chunking strategy
            chunks = []
            chunk_size = self.config.chunk_size
            chunk_overlap = self.config.chunk_overlap

            for i in range(0, len(text_content), chunk_size - chunk_overlap):
                chunk = text_content[i:i + chunk_size]
                if chunk.strip():
                    chunks.append(chunk.strip())

            return {
                "chunks": chunks if chunks else [text_content],
                "metadata": {"fallback_processing": True},
                "has_images": False,
                "multi_modal": False
            }

        except Exception as e:
            logger.error(f"Failed to extract text content (fallback): {e}")
            return {
                "chunks": [],
                "metadata": {"error": str(e)},
                "has_images": False,
                "multi_modal": False
            }
    
    async def _generate_embeddings_with_rag(self, text_chunks: List[str]) -> Optional[List[np.ndarray]]:
        """
        Generate embeddings using existing RAG infrastructure.

        This leverages:
        - Global EmbeddingManager for optimized embedding generation
        - Existing caching and performance optimizations
        - Vision model integration for multi-modal content
        """
        try:
            if self.embedding_manager:
                # ðŸš€ Use existing embedding manager
                embeddings = []

                for chunk in text_chunks:
                    try:
                        # Generate embedding using existing infrastructure
                        embedding = await self.embedding_manager.generate_embedding(chunk)
                        if embedding:
                            embeddings.append(np.array(embedding))
                    except Exception as e:
                        logger.warning(f"Failed to generate embedding for chunk: {e}")
                        continue

                return embeddings if embeddings else None

            elif self.embedding_model:
                # Fallback to basic sentence transformers
                embeddings = self.embedding_model.encode(
                    text_chunks,
                    batch_size=self.config.batch_size,
                    show_progress_bar=False
                )
                return [embedding for embedding in embeddings]
            else:
                logger.warning("No embedding generation capability available")
                return None

        except Exception as e:
            logger.error(f"Failed to generate embeddings with RAG: {e}")
            return None

    async def _generate_query_embedding_with_rag(self, query: str) -> Optional[np.ndarray]:
        """Generate query embedding using existing RAG infrastructure."""
        try:
            if self.embedding_manager:
                # Use existing embedding manager
                embedding = await self.embedding_manager.generate_embedding(query)
                return np.array(embedding) if embedding else None

            elif self.embedding_model:
                # Fallback to basic sentence transformers
                embedding = self.embedding_model.encode([query])[0]
                return embedding
            else:
                logger.warning("No query embedding generation capability available")
                return None

        except Exception as e:
            logger.error(f"Failed to generate query embedding with RAG: {e}")
            return None
    
    def _calculate_cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """Calculate cosine similarity between two vectors."""
        try:
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return float(similarity)
            
        except Exception as e:
            logger.error(f"Failed to calculate cosine similarity: {e}")
            return 0.0
    
    async def remove_session_collection(self, session_id: str) -> bool:
        """Remove a session's vector collection."""
        try:
            if session_id in self.session_collections:
                collection = self.session_collections[session_id]
                doc_count = collection["document_count"]
                
                del self.session_collections[session_id]
                
                self.stats["total_sessions"] -= 1
                self.stats["total_documents"] -= doc_count
                
                logger.info(
                    "ðŸ—‘ï¸ Session vector collection removed",
                    session_id=session_id,
                    documents_removed=doc_count
                )
                
                return True
            
            return False
            
        except Exception as e:
            logger.error(
                "âŒ Failed to remove session collection",
                session_id=session_id,
                error=str(e)
            )
            return False
    
    async def cleanup_expired_sessions(self) -> Dict[str, int]:
        """Clean up expired session collections."""
        try:
            cleanup_stats = {
                "sessions_cleaned": 0,
                "documents_removed": 0,
                "embeddings_freed": 0
            }
            
            current_time = datetime.utcnow()
            expiration_threshold = current_time - session_document_config.expiration.default_workspace_expiration
            
            expired_sessions = []
            
            for session_id, collection in self.session_collections.items():
                if collection["created_at"] < expiration_threshold:
                    expired_sessions.append(session_id)
            
            # Remove expired sessions
            for session_id in expired_sessions:
                collection = self.session_collections[session_id]
                cleanup_stats["documents_removed"] += collection["document_count"]
                cleanup_stats["embeddings_freed"] += sum(
                    len(embeddings) for embeddings in collection["embeddings"].values()
                )
                
                await self.remove_session_collection(session_id)
                cleanup_stats["sessions_cleaned"] += 1
            
            self.stats["cleanup_runs"] += 1
            
            logger.info(
                "ðŸ§¹ Session vector cleanup completed",
                **cleanup_stats
            )
            
            return cleanup_stats
            
        except Exception as e:
            logger.error(f"âŒ Session vector cleanup failed: {e}")
            return {"error": str(e)}
    
    async def get_session_stats(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Get statistics for a specific session."""
        try:
            if session_id not in self.session_collections:
                return None
            
            collection = self.session_collections[session_id]
            
            return {
                "session_id": session_id,
                "collection_name": collection["collection_name"],
                "document_count": collection["document_count"],
                "total_embeddings": sum(len(embeddings) for embeddings in collection["embeddings"].values()),
                "created_at": collection["created_at"].isoformat(),
                "last_accessed": collection["last_accessed"].isoformat(),
                "documents": list(collection["documents"].keys())
            }
            
        except Exception as e:
            logger.error(f"Failed to get session stats: {e}")
            return None
    
    def get_global_stats(self) -> Dict[str, Any]:
        """Get global hybrid vector store statistics."""
        return {
            **self.stats,
            "active_sessions": len(self.session_collections),
            "vector_search_enabled": self.config.enable_vector_search,
            "rag_integration_available": RAG_AVAILABLE,
            "hybrid_architecture": True,
            "components": {
                "embedding_manager": self.embedding_manager is not None,
                "ingestion_pipeline": self.ingestion_pipeline is not None,
                "processor_registry": self.processor_registry is not None,
                "unified_rag": self.unified_rag is not None,
                "fallback_model": self.embedding_model is not None
            },
            "storage_type": "in_memory_only",
            "permanent_storage": False
        }


# Global hybrid session vector store instance
session_vector_store = SessionVectorStore()

logger.info("ðŸ”¥ Revolutionary Hybrid Session Vector Store ready - leveraging existing RAG infrastructure!")
