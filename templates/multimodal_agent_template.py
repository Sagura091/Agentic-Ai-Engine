"""
üé≠ REVOLUTIONARY MULTIMODAL AGENT TEMPLATE
==========================================
Production-ready multimodal agent with vision, text, and advanced processing capabilities.
Copy this file, customize the config section, and launch your multimodal agent!

FEATURES:
‚úÖ Full UnifiedSystemOrchestrator integration
‚úÖ Advanced memory system with multimodal patterns
‚úÖ Complete RAG system with multimodal support
‚úÖ Production multimodal tools (Screenshot analysis, document intelligence, vision)
‚úÖ Autonomous agent with multimodal BDI architecture
‚úÖ Visual understanding and cross-modal reasoning
‚úÖ Real-time learning across modalities
"""

# ================================
# üéõÔ∏è CUSTOMIZE THIS SECTION ONLY
# ================================
AGENT_CONFIG = {
    # ü§ñ Basic Agent Information
    "name": "Multimodal Intelligence Pro",
    "description": "Advanced autonomous multimodal agent with vision, text, and cross-modal reasoning capabilities",
    
    # üß† LLM Configuration
    "llm_provider": "OLLAMA",  # OLLAMA, OPENAI, ANTHROPIC, GOOGLE
    "llm_model": "llama3.2-vision:latest",  # Use vision-capable model
    "temperature": 0.4,  # 0.0 = focused, 1.0 = creative (balanced for multimodal)
    "max_tokens": 8192,  # Longer responses for complex multimodal analysis
    
    # üõ†Ô∏è Production Tools (Real multimodal tools from your system)
    "tools": [
        "screenshot_analysis",             # Visual UI analysis and understanding
        "revolutionary_document_intelligence", # Advanced document processing
        "computer_use_agent",              # Visual computer control
        "browser_automation",              # Visual web interaction
        "web_research",                    # Research with visual content
        "revolutionary_web_scraper",       # Web scraping with image extraction
        "file_system",                     # File operations and management
        "text_processing_nlp",             # NLP and text analysis
        "api_integration",                 # API calls with multimodal data
        "qr_barcode",                      # Visual code generation and reading
        "knowledge_search",                # RAG knowledge search
        "document_ingest"                  # Document ingestion to knowledge base
    ],
    
    # üß† Memory & Learning Configuration
    "memory_type": "ADVANCED",             # NONE, SIMPLE, ADVANCED, AUTO
    "enable_learning": True,               # Learn multimodal patterns
    "enable_rag": True,                   # Use knowledge base with multimodal support
    "enable_collaboration": True,          # Multi-agent collaboration
    
    # ü§ñ Agent Behavior
    "agent_type": "AUTONOMOUS",            # REACT, AUTONOMOUS, RAG, WORKFLOW, etc.
    "autonomy_level": "autonomous",        # reactive, proactive, adaptive, autonomous
    "learning_mode": "active",             # passive, active, reinforcement
    "max_iterations": 200,                 # Max reasoning steps for complex multimodal tasks
    "timeout_seconds": 2400,               # 40 minutes max per multimodal task
    
    # üéØ Multimodal Configuration
    "enable_proactive_behavior": True,     # Agent can suggest multimodal analyses
    "enable_goal_setting": True,          # Agent can set multimodal goals
    "enable_self_modification": True,      # Agent can improve multimodal processing
    "decision_threshold": 0.6,             # Balanced threshold for multimodal decisions
    "cross_modal_reasoning": True,         # Enable cross-modal understanding
    
    # üé≠ Multimodal Specialization
    "modalities": [
        "text", "vision", "documents", "screenshots", "web_content",
        "ui_elements", "charts_graphs", "images", "visual_data"
    ],
    "vision_capabilities": [
        "object_detection", "text_extraction", "ui_understanding",
        "chart_analysis", "image_description", "visual_reasoning",
        "screenshot_analysis", "document_parsing", "visual_search"
    ],
    "cross_modal_tasks": [
        "visual_qa", "image_to_text", "text_to_visual_search",
        "multimodal_summarization", "visual_content_analysis",
        "cross_modal_retrieval", "multimodal_reasoning"
    ],
    
    # üîí Safety & Ethics
    "safety_constraints": [
        "respect_visual_privacy",
        "no_harmful_visual_content",
        "verify_visual_information",
        "maintain_visual_accuracy"
    ],
    "ethical_guidelines": [
        "transparent_visual_analysis",
        "respect_image_rights",
        "accurate_visual_descriptions",
        "responsible_multimodal_ai"
    ]
}

# ‚úçÔ∏è YOUR SYSTEM PROMPT (Customize the personality and behavior)
SYSTEM_PROMPT = """You are an advanced autonomous multimodal agent with exceptional capabilities in visual understanding, text processing, and cross-modal reasoning. You're the ultimate multimodal intelligence system!

üéØ YOUR MISSION:
Process and understand information across multiple modalities (text, vision, documents, screenshots) and provide comprehensive multimodal analysis and insights.

üé≠ YOUR MULTIMODAL CAPABILITIES:
- Advanced visual understanding and analysis
- Screenshot analysis and UI comprehension
- Document intelligence with visual elements
- Cross-modal reasoning and synthesis
- Visual question answering
- Image description and interpretation
- Chart and graph analysis
- Multimodal content creation planning
- Visual search and retrieval

üîç YOUR MULTIMODAL METHODOLOGY:
1. **Input Analysis**: Identify and process different modalities
2. **Visual Understanding**: Analyze visual content with deep comprehension
3. **Text Processing**: Extract and understand textual information
4. **Cross-Modal Fusion**: Combine insights across modalities
5. **Reasoning**: Apply multimodal reasoning for complex understanding
6. **Synthesis**: Generate comprehensive multimodal insights
7. **Verification**: Cross-check information across modalities
8. **Knowledge Integration**: Store multimodal insights for future use

üé≠ YOUR PERSONALITY:
- Perceptive and detail-oriented across all modalities
- Analytical and systematic in multimodal processing
- Creative in cross-modal connections
- Precise in visual descriptions
- Thorough in multimodal analysis
- Ethical in handling visual content

üõ†Ô∏è YOUR TOOLS:
You have access to revolutionary multimodal tools including screenshot analysis, visual computer control, document intelligence, and a complete multimodal knowledge management system.

üëÅÔ∏è YOUR VISUAL UNDERSTANDING:
- Accurate object and element detection
- Comprehensive scene understanding
- UI element identification and interaction
- Text extraction from images and screenshots
- Chart and graph interpretation
- Visual reasoning and inference
- Cross-modal content correlation

üåü YOUR SPECIAL ABILITIES:
- Simultaneous processing of multiple modalities
- Visual-textual cross-referencing and validation
- Multimodal pattern recognition
- Visual workflow understanding and automation
- Cross-modal knowledge synthesis
- Multimodal content generation planning

üîí YOUR MULTIMODAL ETHICS:
- Respect privacy in visual content
- Provide accurate visual descriptions
- Handle sensitive visual information appropriately
- Maintain transparency in multimodal analysis
- Respect intellectual property in visual content
- Follow responsible AI practices across all modalities

Remember: Great multimodal intelligence is about understanding the rich connections between different types of information and providing insights that no single modality could achieve alone!"""

# ================================
# üöÄ LAUNCH CODE (DON'T TOUCH!)
# ================================
import asyncio
import sys
import uuid
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Import the complete production infrastructure
from app.core.unified_system_orchestrator import get_enhanced_system_orchestrator
from app.agents.factory import AgentBuilderFactory, AgentBuilderConfig, AgentType, MemoryType
from app.agents.base.agent import AgentCapability
from app.agents.autonomous import AutonomousLangGraphAgent, AutonomyLevel, LearningMode, create_autonomous_agent
from app.llm.models import LLMConfig, ProviderType
from app.llm.manager import LLMProviderManager
from app.memory.unified_memory_system import UnifiedMemorySystem
from app.rag.core.unified_rag_system import UnifiedRAGSystem
from app.tools.unified_tool_repository import UnifiedToolRepository

import structlog
logger = structlog.get_logger(__name__)

class MultimodalAgentLauncher:
    """Production-ready multimodal agent launcher using complete infrastructure."""
    
    def __init__(self):
        self.orchestrator = None
        self.agent = None
        self.agent_id = f"multimodal_agent_{uuid.uuid4().hex[:8]}"
        
    async def initialize_infrastructure(self):
        """Initialize the complete production infrastructure."""
        try:
            print("üîß Initializing Revolutionary Agentic AI Infrastructure...")
            
            # Get the enhanced system orchestrator (THE unified system)
            self.orchestrator = get_enhanced_system_orchestrator()
            await self.orchestrator.initialize()
            
            print("‚úÖ Infrastructure initialized successfully!")
            print(f"   üß† Memory System: {type(self.orchestrator.memory_system).__name__}")
            print(f"   üìö RAG System: {type(self.orchestrator.unified_rag).__name__}")
            print(f"   üõ†Ô∏è Tool Repository: {self.orchestrator.tool_repository.stats['total_tools']} tools available")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Infrastructure initialization failed: {str(e)}")
            logger.error("Infrastructure initialization failed", error=str(e))
            return False
    
    async def create_multimodal_agent(self):
        """Create the multimodal agent using production configuration."""
        try:
            print(f"üé≠ Creating {AGENT_CONFIG['name']}...")
            
            # Create LLM configuration
            llm_config = LLMConfig(
                provider=ProviderType(AGENT_CONFIG["llm_provider"]),
                model_id=AGENT_CONFIG["llm_model"],
                temperature=AGENT_CONFIG["temperature"],
                max_tokens=AGENT_CONFIG["max_tokens"]
            )
            
            # Initialize LLM manager and create LLM instance
            llm_manager = LLMProviderManager()
            await llm_manager.initialize()
            llm = await llm_manager.create_llm_instance(llm_config)
            
            # Get tools from the unified tool repository
            tools = []
            for tool_name in AGENT_CONFIG["tools"]:
                tool = self.orchestrator.tool_repository.get_tool(tool_name)
                if tool:
                    tools.append(tool)
                    print(f"   ‚úÖ Tool loaded: {tool_name}")
                else:
                    print(f"   ‚ö†Ô∏è Tool not found: {tool_name}")
            
            print(f"   üõ†Ô∏è {len(tools)} tools loaded successfully")
            
            # Create autonomous agent using the factory function
            self.agent = create_autonomous_agent(
                name=AGENT_CONFIG["name"],
                description=AGENT_CONFIG["description"],
                llm=llm,
                tools=tools,
                autonomy_level=AGENT_CONFIG["autonomy_level"],
                learning_mode=AGENT_CONFIG["learning_mode"],
                agent_id=self.agent_id,
                enable_proactive_behavior=AGENT_CONFIG["enable_proactive_behavior"],
                enable_goal_setting=AGENT_CONFIG["enable_goal_setting"],
                enable_self_modification=AGENT_CONFIG["enable_self_modification"],
                decision_threshold=AGENT_CONFIG["decision_threshold"],
                safety_constraints=AGENT_CONFIG["safety_constraints"],
                ethical_guidelines=AGENT_CONFIG["ethical_guidelines"],
                max_iterations=AGENT_CONFIG["max_iterations"],
                timeout_seconds=AGENT_CONFIG["timeout_seconds"]
            )
            
            # Set the system prompt
            self.agent.system_prompt = SYSTEM_PROMPT
            
            # Initialize agent memory and RAG
            if AGENT_CONFIG["enable_rag"]:
                await self.orchestrator.unified_rag.create_agent_knowledge_base(self.agent_id)
                print("   üìö Knowledge base created")
            
            await self.orchestrator.memory_system.create_agent_memory(self.agent_id)
            print("   üß† Memory system initialized")
            
            # Create tool profile for the agent
            await self.orchestrator.tool_repository.create_agent_profile(self.agent_id)
            for tool_name in AGENT_CONFIG["tools"]:
                await self.orchestrator.tool_repository.assign_tool_to_agent(self.agent_id, tool_name)
            print("   üõ†Ô∏è Tool profile created")
            
            print(f"‚úÖ {AGENT_CONFIG['name']} created successfully!")
            return True
            
        except Exception as e:
            print(f"‚ùå Agent creation failed: {str(e)}")
            logger.error("Agent creation failed", error=str(e))
            return False
    
    async def run_interactive_session(self):
        """Run interactive multimodal session with the agent."""
        print("\n" + "="*60)
        print(f"üé≠ {AGENT_CONFIG['name']} - Interactive Multimodal Intelligence")
        print("="*60)
        print(f"üß† LLM: {AGENT_CONFIG['llm_provider']} - {AGENT_CONFIG['llm_model']}")
        print(f"üõ†Ô∏è Tools: {len(AGENT_CONFIG['tools'])} multimodal tools available")
        print(f"üìö RAG: {'Enabled' if AGENT_CONFIG['enable_rag'] else 'Disabled'}")
        print(f"üéØ Autonomy: {AGENT_CONFIG['autonomy_level']} level")
        print(f"üëÅÔ∏è Modalities: {', '.join(AGENT_CONFIG['modalities'][:5])}...")
        print("\nüí° Multimodal Intelligence Tips:")
        print("   ‚Ä¢ Ask for screenshot analysis and UI understanding")
        print("   ‚Ä¢ Request document analysis with visual elements")
        print("   ‚Ä¢ Ask for visual question answering")
        print("   ‚Ä¢ Request cross-modal reasoning tasks")
        print("   ‚Ä¢ Ask for visual content description and analysis")
        print("   ‚Ä¢ Type 'quit' to exit")
        print("\nüëÅÔ∏è Vision: Advanced visual understanding and cross-modal reasoning enabled")
        print("\n" + "="*60)
        
        while True:
            try:
                user_input = input("\nüßë Multimodal Request: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'bye']:
                    print("üëã Multimodal intelligence session ended. All insights saved!")
                    break
                
                if not user_input:
                    continue
                
                print(f"\nüé≠ {AGENT_CONFIG['name']}: Processing multimodal request...")
                
                # Execute multimodal task
                result = await self.agent.execute(
                    task=user_input,
                    context={
                        "multimodal_mode": True,
                        "save_to_knowledge_base": AGENT_CONFIG["enable_rag"],
                        "learning_enabled": AGENT_CONFIG["enable_learning"],
                        "cross_modal_reasoning": AGENT_CONFIG["cross_modal_reasoning"],
                        "modalities": AGENT_CONFIG["modalities"],
                        "vision_capabilities": AGENT_CONFIG["vision_capabilities"],
                        "cross_modal_tasks": AGENT_CONFIG["cross_modal_tasks"],
                        "safety_constraints": AGENT_CONFIG["safety_constraints"],
                        "timestamp": datetime.now().isoformat()
                    }
                )
                
                if result:
                    if isinstance(result, dict) and 'response' in result:
                        print(f"\nüëÅÔ∏è {AGENT_CONFIG['name']}:")
                        print(result['response'])
                    else:
                        print(f"\nüëÅÔ∏è {AGENT_CONFIG['name']}: Multimodal analysis completed!")
                        if hasattr(result, 'content'):
                            print(result.content)
                else:
                    print(f"\nüëÅÔ∏è {AGENT_CONFIG['name']}: Multimodal task completed successfully!")
                
            except KeyboardInterrupt:
                print("\nüëã Multimodal session interrupted. Goodbye!")
                break
            except Exception as e:
                print(f"\n‚ùå Error during multimodal processing: {str(e)}")
                logger.error("Multimodal processing execution error", error=str(e))

async def main():
    """Main launcher function."""
    print("üé≠ REVOLUTIONARY MULTIMODAL AGENT TEMPLATE")
    print("=" * 50)
    
    launcher = MultimodalAgentLauncher()
    
    # Initialize infrastructure
    if not await launcher.initialize_infrastructure():
        return
    
    # Create multimodal agent
    if not await launcher.create_multimodal_agent():
        return
    
    # Run interactive session
    await launcher.run_interactive_session()

if __name__ == "__main__":
    """Launch the Revolutionary Multimodal Agent."""
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nüëã Goodbye!")
    except Exception as e:
        print(f"\n‚ùå Fatal error: {str(e)}")
        logger.error("Fatal error in multimodal agent", error=str(e))
